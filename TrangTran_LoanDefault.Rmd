---
Title: "Loan Default"
Analyst: Trang Tran
---

# Load in the data
```{r}
setwd("C:/Users/trang/Downloads/Practice Datasets/LoanDefault")
data = read.csv("Loan_default.csv", header = T)
```

Analysis plan:
1. Data Inspection
2. Data Cleaning
3. Visualization
4. Hypothesis testing
5. Logistic regression
6. Supervised learning models
7. Model Evaluation

# 1. Data Inspection
```{r}
names(data) # displays features

# "LoanID"         "Age"            "Income"         "LoanAmount"     "CreditScore"    "MonthsEmployed"
# "NumCreditLines" "InterestRate"   "LoanTerm"       "DTIRatio"       "Education"      "EmploymentType"
# "MaritalStatus"  "HasMortgage"    "HasDependents"  "LoanPurpose"    "HasCoSigner"    "Default

nrow(data) # 255347 rows

summary(data) # identify data types and summary 
```

# 2. Data Cleaning
```{r}
# Detect missing values
missing_values = colSums(is.na(data)) # no missing values in any column

# Detect duplicates
data[,duplicated(data$LoanID)] # no duplicated account

# Categorize numeric and categorical variables
numerical_vars = c("Age","Income","LoanAmount","CreditScore","MonthsEmployed","NumCreditLines","InterestRate","LoanTerm","DTIRatio") # numeric features
cat_vars = c("Education","EmploymentType","MaritalStatus","HasMortgage","HasDependents","LoanPurpose","HasCoSigner") #categorical features

# Use IQR method to detect outliers
par(mfrow=c(1,2))
for (column in numerical_vars) {
  quants = quantile(data[[column]])
  IQR = quants[4] - quants[2]
  L = quants[2] - 1.5 * IQR
  U = quants[4] + 1.5 * IQR
  data_IQR = data[data[[column]] >= L & data[[column]] <= U, ]
  boxplot(data_IQR[[column]], ylab=column, width=0.5)
}

# Result: There appears to be no outliers
```

# 3. Data Visualization
```{r}
# Explore patterns or trends in the distribution of numerical features between defaulters and non-defaulters
par(mfrow = c(1, 2))
# Loop through each variable and create side-by-side boxplots for non-defaulters and defaulters
for (variable in numerical_vars) {
  boxplot(data[[variable]][data$Default == 0], main = paste("Non-Defaulters -", variable), col = "blue", ylab = paste(variable))
  boxplot(data[[variable]][data$Default == 1], main = paste("Defaulters -", variable), col = "red", ylab = paste(variable))
}

# Comment: Looking at the graphs, it is quite evident that defaulters and non-defaulters exhibit distinct distributions in nearly all numerical variables, especially in median values, with the exception of Loan Term. It could be a hint about for subsequent hypothesis tests.
```


# 4. Hypothesis Testing to analyze differences across groups 
# Chi-square tests for categorical variables with Default
```{r}
# Create a loop to perform chi-square test to see relationship between categorical variables and Default
for (feature in cat_vars) {
  contingency_table = table(data[,feature], data$Default)
  result = chisq.test(contingency_table)
  print(paste("Chi-squared test for", feature, "vs Default"))
  print(result)
}
# The chi-square tests of all categorical features show p-value less than 0.05, concluding that there is an association between any categorical variable in question and whether the corresponding borrower defaults on their loan. 

# Condition: the number of count in each category in each group is more than 5. Condition is met. 
```

# T-test to see how mean of each numerical features differ between people who default and who don't
```{r}
t.test(data=data, LoanTerm ~ Default, alternative="two.sided",conf.level=0.95)  # p-value = 0.7834 --> fail to reject
t.test(data=data, CreditScore ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16  
t.test(data=data, NumCreditLines ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16 
t.test(data=data, MonthsEmployed ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16 
t.test(data=data, InterestRate ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16 
t.test(data=data, DTIRatio ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16
t.test(data=data, Income ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16
t.test(data=data, LoanAmount ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16
t.test(data=data, Age ~ Default, alternative="two.sided",conf.level=0.95) # p-value < 2.2e-16

# Comment: Except for LoanTerm, which shows a p-value > 0.05, all other t-tests reveal an extremely small p-value, indicating strong evidence that there is a significant difference in the mean distribution of the respective numerical variables between defaulters and non-defaulters.

# To ensure t-tests are applicable and useful, certain conditions must be met
# 1. Independence: Each data row corresponds to a different borrower; borrowers are independent of each other.  
# 2. Normality
par(mfrow = c(1,2))
for (feature in numerical_vars) {
  qqnorm(data[[feature]][data$Default == 0], main=paste("Q-Q Plot for", feature, "- Non-Default"))
  qqnorm(data[[feature]][data$Default == 1], main=paste("Q-Q Plot for", feature, "- Default"))
}
par(mfrow = c(1,1))
# Comment: the QQ-plot of each t-test for distribution of each numerical feature looks linear, except for that of LoanTerm. That perhaps explains the result of its t-test. Data on LoanTerm is not fit to conduct hypothesis test. 

```

# 5. Logistic Regression
```{r}
# Use logistic regression
log_model = glm(data=data, Default ~ Age + Income + LoanAmount + CreditScore + MonthsEmployed + NumCreditLines + InterestRate + DTIRatio + Education + EmploymentType + MaritalStatus + HasMortgage + HasDependents + LoanPurpose + HasCoSigner, family="binomial")
summary(log_model)

# LoanPurposeEducation and LoanPurposeOther have p-value higher than 0.05, so they're not significant in predicting Default status of a borrower. In other words, if a borrower takes out a loan for Education purpose or purpose not specified, those factors aren't statistically significant in predicting whether that borrower defaults or not. Thus, I'm excluding them in my equation model. 

# Equation: P(Default) = e^(-0.47 - 0.039*Age - 0.000008*Income + 0.000004*LoanAmount - 0.00076*CreditScore - 0.00*MonthsEmployed + 0.087*NumCreditLines + 0.0069*InterestRate + 0.0281*DTIRatio + 0.078*if(Education: High School) - 0.132*if(Education:Master's) - 0.179*if(Education: PhD) + 0.28*if(Employment: Part-time) + 0.236*if(Employment: Self-employed) + 0.444*if(Employment: Unemployed) - 0.23*if(Marital Status: Married) - 0.066*if(Marital Status: Single) - 0.157*if(Mortgage:Yes) - 0.243*if(Dependents:Yes) + 0.043*if(Loan Purpose:Business) - 0.195*if(Loan Purpose:Home) - 0.271*if(CoSigner:Yes))/(1+e^(-0.47 - 0.039*Age - 0.000008*Income + 0.000004*LoanAmount - 0.00076*CreditScore - 0.00*MonthsEmployed + 0.087*NumCreditLines + 0.0069*InterestRate + 0.0281*DTIRatio + 0.078*if(Education: High School) - 0.132*if(Education:Master's) - 0.179*if(Education: PhD) + 0.28*if(Employment: Part-time) + 0.236*if(Employment: Self-employed) + 0.444*if(Employment: Unemployed) - 0.23*if(Marital Status: Married) - 0.066*if(Marital Status: Single) - 0.157*if(Mortgage:Yes) - 0.243*if(Dependents:Yes) + 0.043*if(Loan Purpose:Business) - 0.195*if(Loan Purpose:Home) - 0.271*if(CoSigner:Yes)))

# Visualize fit
library(pROC)
roc_obj = roc(data$Default, fitted(log_model))
roc_obj$auc # auc = 0.748
plot(roc_obj, main = paste("Area under the Curve is", round(roc_obj$auc,3)))

# Comment: With AUC = 0.748, this logistic regression model is only decent at distinguishing between defaulters and non-defaulters. A model considered "good" should have AUC equal to at least 0.8.
```
# 6. Supervised Learning models

# Data processing
```{r}
# Standardize data
data.std = data
data.std[,numerical_vars] = scale(data[,numerical_vars])

# Split data into training set and testing set
data.std = data.std[sample(nrow(data.std)),]
training_set = data.std[1:192000,]
testing_set = data.std[192001:255347,]
```

# Predicting with Logistic Regression!
```{r}
# Logistic Regression
log_reg = glm(data=training_set, Default ~ Age + Income + LoanAmount + CreditScore + MonthsEmployed + InterestRate + HasMortgage + HasCoSigner, family = "binomial")

thresh_ac = crossval_log_reg(training_set, log_reg, training_set$Default, 5) 
plot((1:100)/100, thresh_ac, main = "Accuracy for different choices of T, 5-cross validated", xlab = "Threshold (T)", ylab = "Accuracy")
best_thres = which(thresh_ac == max(thresh_ac))/100 # best threshold is 0.47

log_preds = predict(log_reg, testing_set, type="response")
log_conf = table(testing_set$Default, log_preds > best_thres)
#    FALSE  TRUE
#  0 55887   202
#  1  7008   250

# Sensitivity = 250/(250+7008) = 0.0344
# Specificity = 55887/(55887+202) = 0.9963986
# Accuracy = (55887+250)/(55887+250+202+7008) = 0.8861825
```

# Predicting with kNN!
```{r}
# For kNN, we need to encode categorical variables
# Install the required package
#install.packages("fastDummies")

# Load dummy library
library(fastDummies)

data_encoded = data.std # make a copy
data_encoded = dummy_cols(data_encoded, select_columns = cat_vars) # dummy encode
data_encoded = data_encoded[, -which(names(data_encoded) %in% cat_vars)] # remove original categorical columns
data_encoded = data_encoded[, c(setdiff(seq_along(data_encoded), which(names(data_encoded) == "Default")), which(names(data_encoded) == "Default"))] # move the Default column to the last position

data_encoded = data_encoded[sample(nrow(data_encoded)),] # shuffle data
training_encoded = data_encoded[1:192000,] # split data into training and testing set
testing_encoded = data_encoded[192001:255347,]

# Building kNN model
library(class)

# k = 3
knn_3 = knn(training_encoded[,2:(ncol(data_encoded)-1)], testing_encoded[,2:(ncol(data_encoded)-1)], training_encoded$Default, 3)
knn_3_conf = table(testing_encoded$Default, knn_3)
#        0     1    # confidence matrix for k = 3
#  0 53839  2206
#  1  6538   764

# Sensitivity = (764)/(764+6538) = 0.105
# Specificity = 53839/(53839+2206) = 0.961
# Accuracy = 0.862

# k = 5
knn_5 = knn(training_encoded[,2:(ncol(data_encoded)-1)], testing_encoded[,2:(ncol(data_encoded)-1)], training_encoded$Default, 5)
knn_5_conf = table(testing_encoded$Default, knn_5)
#        0     1     # confidence matrix for k = 5
#  0 54890  1155
#  1  6784   518

# Sensitivity = 0.071
# Specificity = 0.979
# Accuracy = 0.875

# k = 8
knn_8 = knn(training_encoded[,2:(ncol(data_encoded)-1)], testing_encoded[,2:(ncol(data_encoded)-1)], training_encoded$Default, 8)
knn_8_conf = table(testing_encoded$Default, knn_8)
#        0     1     # confidence matrix for k = 5
#  0 55343   702
#  1  6937   365

# Sensitivity = 0.049
# Specificity = 0.987
# Accuracy = 0.879
```

# 7. Model Evaluation:
Both the logistic regression model and the three KNN models, despite showcasing high accuracy, lack practical utility when it comes to predicting loan defaults. In the context of banking, where machine learning models aim to identify customers at risk of defaulting in the future, the most valuable model should excel in capturing true defaults among all actual defaults. This ensures that when applied to new data, the model accurately identifies a significant proportion of true default cases. In simpler terms, the priority is on a model that achieves a high sensitivity rate. Unfortunately, the models analyzed in this study show an exceptionally low sensitivity rate of about 3-4%, indicating their limited ability to detect true defaultsâ€”only capturing a small fraction of all instances.

# Run this first: Cross validation method
```{r}
library(class)
# Function that performs cross validation for knn
crossval_knn = function(data, labels, folds, max_k){
  # SYNTAX
  # data is the data.frame of training_data (only the vector used!)
  # labels is the vector of class labels
  # folds is the number of folds validation to do
  # max_k is the upper limit on the k to use
  
  acc_knn = rep(0, max_k)
  for(k in 1:max_k){
    results = rep(0, folds)
    for(i in 1:folds){
      ind = seq(i,length(data[,1]), folds)
      training_set = data[-ind, ]
      testing_set = data[ind,]
      training_labels = labels[-ind]
      testing_labels = labels[ind]
      
      predicted_labels = knn(training_set, testing_set, training_labels, k)
      
      results[i] = sum(diag(table(predicted_labels, testing_labels)))/length(testing_labels)
    }
    acc_knn[k] = mean(results)
  }
  return(acc_knn) # returns the vector of average performance.
}

crossval_log_reg = function(data, model, labels, folds){
  # SYNTAX
  # data
  # model is the log_reg_model
  # labels is the vector of class labels
  # folds is the number of folds validation to do

  acc_T = rep(0, 100)
  for(k in 1:100){
    results = rep(0, folds)
    for(i in 1:folds){
      ind = seq(i,length(labels), folds)
      training_set = data[-ind, ]
      testing_set = data[ind,]
      training_labels = labels[-ind]
      testing_labels = labels[ind]
      
      predicted_labels = predict.glm(model, testing_set, type = "response") > k/100
      
      results[i] = sum(diag(table(predicted_labels, testing_labels)))/length(testing_labels)
    }
    acc_T[k] = mean(results)
  }
  return(acc_T) # returns the vector of average performance for each threshold.
}
```

